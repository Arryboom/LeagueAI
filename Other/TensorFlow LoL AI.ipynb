{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:\n",
    "http://www.scholarpedia.org/article/Policy_gradient_methods#Likelihood_Ratio_Methods_and_REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pseudo code for finite-difference policy gradient\n",
    "\n",
    "input: policy parameterization θh\n",
    "for i=1 to  I do \n",
    "    generate policy variation Δθi\n",
    "    estimate J^i≈J(θh+Δθi)=⟨∑Hk=0akrk⟩ from roll-out\n",
    "    estimate J^ref , e.g., J^ref=J(θh−Δθi) from roll-out\n",
    "    compute ΔJ^i≈J(θh+Δθi)−Jref\n",
    "end for \n",
    "return gradient estimate gFD=(ΔΘTΔΘ)−1ΔΘTΔJ^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class template set.\n"
     ]
    }
   ],
   "source": [
    "class State:\n",
    "    \n",
    "    hp = 1.0\n",
    "    \n",
    "    cloest_minion_dist = 0\n",
    "    tower_dist = 0\n",
    "    \n",
    "    # game time counted in iterations or steps\n",
    "    game_time = 0\n",
    "\n",
    "state = State()    \n",
    "\n",
    "\n",
    "class Policy:\n",
    "    \n",
    "    c_max_visible_dist = 20.0\n",
    "    c_close_dist_cutoff = 1.0 / 10.0\n",
    "    c_tower_safe_dist_cutoff = c_max_visible_dist / 3\n",
    "    c_max_hp = 1\n",
    "    c_game_time_beginning_cutoff = 0.5 *60 / 5\n",
    "    \n",
    "    logistic_func_x_scale = 6\n",
    "    \n",
    "    # theta for the action:\n",
    "    # atk minion, atk tower, move to goal, retreat\n",
    "    # respectively\n",
    "    theta = np.zeros( (4,1) )\n",
    "    theta[0] = c_close_dist_cutoff\n",
    "    theta[1] = 1\n",
    "    theta[2] = 0.8 / (45*60/5)\n",
    "    theta[3] = 0.2\n",
    "    \n",
    "    \n",
    "    def atkMinionPFunc(self, x):\n",
    "        # map the dist to range in [0,1]\n",
    "        hp_frac = x.hp / c_max_hp\n",
    "        cloest_minion_dist = x.cloest_minion_dist / c_max_visible_dist\n",
    "        \n",
    "        p = theta[0]*hp_frac / cloest_minion_dist\n",
    "        if   p > 1: return 1\n",
    "        else p < 0: return 0\n",
    "        else      : return p\n",
    "    \n",
    "    \n",
    "    def atkTowerPFunc(self, tower_dist):\n",
    "        # map the dist to range in [0,1]\n",
    "        hp_frac = x.hp / c_max_hp\n",
    "        \n",
    "        #####TODO##########################\n",
    "        # the range is incorrect \n",
    "        #tower_dist = x.tower_dist / c_max_visible_dist * logistic_func_x_scale\n",
    "        \n",
    "        #p = 1/ ( 1 + exp( -(theta[1]*hp_frac) * (tower_dist - c_tower_safe_dist_cutoff) ) )\n",
    "        p = theta[1]*hp_frac*(1/( 1+exp( -2*(x+1)) ) - 1/( 1+exp( -10*(x-3)) ) )\n",
    "        if   p > 1: return 1\n",
    "        else p < 0: return 0\n",
    "        else      : return p\n",
    "\n",
    "        \n",
    "    def mvToGoalPFunc(self, x):\n",
    "        if x.game_time < c_game_time_beginning_cutoff:\n",
    "            return 1\n",
    "        else:\n",
    "            p = theta[2] * x.game_time\n",
    "            if   p > 1: return 1\n",
    "            else p < 0: return 0\n",
    "            else      : return p\n",
    "\n",
    "            \n",
    "    def mvRetreatPFunc(self, x):\n",
    "        hp_frac = x.hp / c_max_hp\n",
    "     \n",
    "        p = exp( theta[3] / hp_frac ) - 1   \n",
    "        if   p > 1: return 1\n",
    "        else p < 0: return 0\n",
    "        else      : return p\n",
    "        \n",
    "    \n",
    "    def varPolicy(self, policy_gradient):\n",
    "        return 0.01*policy_gradient*self.theta\n",
    " \n",
    "\n",
    "    def actRollout(self, x):\n",
    "        AM = self.atkMinionPFunc(x)\n",
    "        AT = self.atkTowerPFunc(x)\n",
    "        MG = self.mvToGoalPFunc(x)\n",
    "        MR = self.mvRetreatPFunc(x)\n",
    "        \n",
    "        TOT = AM + AT + MG + MR\n",
    "        AM_cutoff = AM / TOT\n",
    "        AT_cutoff = AM + AT / TOT\n",
    "        MG_cutoff = AM + AT + MG / TOT\n",
    "        #MR_cutoff = MR / TOT\n",
    "        \n",
    "        ran = random.random()\n",
    "        \n",
    "        if   ran < AM_cutoff: return 0\n",
    "        elif ran < AT_cutoff: return 1\n",
    "        elif ran < MG_cutoff: return 2\n",
    "        else                : return 3\n",
    "    \n",
    "    p_rollout_array = np.zeros( (4,1) )\n",
    "    def pRollout(self, x):\n",
    "        AM = self.atkMinionPFunc(x)\n",
    "        AT = self.atkTowerPFunc(x)\n",
    "        MG = self.mvToGoalPFunc(x)\n",
    "        MR = self.mvRetreatPFunc(x)\n",
    "        \n",
    "        TOT = AM + AT + MG + MR\n",
    "        p_rollout_array[0] = AM / TOT\n",
    "        p_rollout_array[1] = AT / TOT\n",
    "        p_rollout_array[2] = MG / TOT\n",
    "        p_rollout_array[3] = MR / TOT\n",
    "        return p_rollout_array\n",
    "        \n",
    "policy = Policy()\n",
    "        \n",
    "print(\"class template set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieveStepReward(state, policy):\n",
    "    static \n",
    "    # expected to implement these:\n",
    "    #\n",
    "    # sample an action based on the given policy\n",
    "    # execute the action\n",
    "    # retrieve the reward\n",
    "    \n",
    "    # for now we will just sample from random number for testing purpose\n",
    "    #\n",
    "    reward = np.randn( policy.rollout() )\n",
    "    \n",
    "    # artificial reward for\n",
    "    # attack minion, attack tower, move to base, move to retreat\n",
    "    np.dot( policy.pRollout(),  retrieveStepReward.std_reward )\n",
    "    \n",
    "    return    \n",
    "retrieveStepReward.std_reward = np.array(1, 10, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateR_i(policy_grad, delta_theta):\n",
    "    R_i = updateR_i.R_i\n",
    "    a = updateR_i.a\n",
    "    \n",
    "    R_i = (1-a)*R_i + a*np.dot(policy_grad, delta_theta)\n",
    "    updateR_i.R_i = R_i\n",
    "    return R_i\n",
    "    \n",
    "updateR_i.R_i = 1\n",
    "updateR_i.a = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateR_ref(reward):\n",
    "    R_ref = updateR_ref.R_ref\n",
    "    a = updateR_ref.a\n",
    "    \n",
    "    R_ref = (1-a)*R_ref + a*reward\n",
    "    updateR_ref = R_ref\n",
    "    return R_ref\n",
    "\n",
    "updateR_ref.R_ref = 1\n",
    "updateR_ref.a = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policyGradByFD(policy, c_numSamples = 10)\n",
    "\"\"\" This function returns the estimate of policy gradient \n",
    "    calculated by finite difference.\n",
    "    \n",
    "    Expect:\n",
    "        c_numPolicyParm: a global variable, number of policy parameter\n",
    "    \n",
    "    Args:\n",
    "        theta: current policy parameter, numpy.ndarray of size (n, 1)\n",
    "        c_numSamples: int. Number of samples to take (or equivalently number of \n",
    "                      policy parameter perturbation to make)\n",
    "\n",
    "    Returns:\n",
    "        policyGrad: policy gradient estimate, numpy.ndarray of size (n, 1)\n",
    "                    calculated using least square formula\n",
    "                    x = (A^T A)A^T b\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "c_numPolicyParm = policy.theta.size()\n",
    "\n",
    "# preallocate vectors\n",
    "delta_theta = np.zeros( (c_numSamples, c_numPolicyParm) )\n",
    "delta_R = np.zeros( (c_numSamples, 1) )\n",
    "\n",
    "for i in range(c_numSamples):\n",
    "    [theta_i, delta_theta_i] = policy.varPolicy()\n",
    "    r=retrieveStepReward()\n",
    "    updateR_i(policy_grad, delta_theta_i)    # currently by running mean\n",
    "    updateR_ref(r)  # currently by running mean\n",
    "    delta_R_i = R_i - R_ref\n",
    "    \n",
    "    delta_theta[i] = delta_theta_i\n",
    "    delta_R[i] = delta_R_i\n",
    "    \n",
    "\n",
    "policy_grad = inv(delta_theta.T * delta_theta) * delta_theta.T * delta_R\n",
    "policyGradByFD.policy_grad = policy_grad\n",
    "return policy_grad\n",
    "\n",
    "policyGradByFD.policy_grad = np.array([1, 1, 1, 1])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to estimate J for policy gradient to work\n",
    "\n",
    "J will be estimated using TD(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
