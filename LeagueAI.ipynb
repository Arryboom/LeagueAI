{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use(\"Qt5Agg\")\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "from mss import mss\n",
    "import math\n",
    "from math import exp\n",
    "from random import randint\n",
    "from random import random\n",
    "from numpy.linalg import inv\n",
    "\n",
    "\n",
    "\n",
    "#mouse control and window dimensions\n",
    "SCREEN_WIDTH = 1366\n",
    "SCREEN_HEIGHT = 768\n",
    "MONITOR_WIDTH = 800\n",
    "MONITOR_HEIGHT = 600\n",
    "import win32api, win32con\n",
    "#Player Control\n",
    "click_cooldown = 0.5\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select object detection model from the same folder as this script\n",
    "MODEL_NAME = 'LeagueAI_v3'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('training', 'LeagueAI_v2.pbtxt')\n",
    "# Number of classes in the pbtxt file that can be detected by the model\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the frozen model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper classes for various champion functions and interaction with the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# click a postion in the screen using x,y coordinates in pixels\n",
    "def click(x,y, attack):\n",
    "    win32api.SetCursorPos((x,y))\n",
    "    if attack == True:\n",
    "        win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,x,y,0,0)\n",
    "        win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP,x,y,0,0)\n",
    "    else:\n",
    "        win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTDOWN,x,y,0,0)\n",
    "        win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTUP,x,y,0,0)\n",
    "# move the mouse cursor to x,y coordinates in pixels\n",
    "def move_cursor_to(x,y):\n",
    "    win32api.SetCursorPos((x,y))\n",
    "# find the player character position marked as x,y (converting the boxes to a single point position)\n",
    "# some hard coded things are done here: the character model center is not in the center of the detection\n",
    "# but in the lower part near its feet. Thats why we determined some factors for calculating the position\n",
    "# from the box experimentally\n",
    "def find_box_xy(box):\n",
    "    playerwidth =  (box[3] - box[1])\n",
    "    playerheight = box[2] - box[0]\n",
    "    playerpos_y = (box[0] + playerheight/1.3)\n",
    "    playerpos_x = (box[1] + playerwidth/2)\n",
    "    return([playerpos_y, playerpos_x, playerheight, playerwidth])\n",
    "# find out in which grid zell on object is\n",
    "def find_object_state(grid, minion_origin, player_origin, grid_width, grid_height):\n",
    "    x_object = (minion_origin[0])#/MONITOR_WIDTH)#*SCREEN_WIDTH\n",
    "    y_object = (minion_origin[1])#/MONITOR_HEIGHT)#*SCREEN_HEIGHT\n",
    "    x_player = (player_origin[0])#/MONITOR_WIDTH)#*SCREEN_WIDTH\n",
    "    y_player = (player_origin[1])#/MONITOR_HEIGHT)#*SCREEN_HEIGHT\n",
    "    #find difference to player in pixels\n",
    "    x_dif = -1*(x_player - x_object)\n",
    "    y_dif = (y_player - y_object)    \n",
    "    #transform pixels into states\n",
    "    state_x = round(x_dif/grid_width)\n",
    "    state_y = round(y_dif/grid_height)\n",
    "    return([state_x, state_y])\n",
    "# click a certain grid cell given by x and y coordinate in the state grid\n",
    "# the click will click in the center of the grid\n",
    "def click_state(state_x, state_y, state_width, state_height, player_origin, attack):\n",
    "    x_click = (player_origin[0]/MONITOR_WIDTH)*SCREEN_WIDTH + state_x*(state_width/MONITOR_WIDTH)*SCREEN_WIDTH\n",
    "    y_click = (player_origin[1]/MONITOR_HEIGHT)*SCREEN_HEIGHT + (-1*state_y*(state_height/MONITOR_HEIGHT)*SCREEN_HEIGHT)\n",
    "    click(int(x_click), int(y_click), attack)\n",
    "    #move_cursor_to(int(x_click), int(y_click))\n",
    "# set a certain state in the grid to a certain value to mark for example the type of unit in the grid cell\n",
    "def set_array_pos(grid, x, y_in, value, x_grid_size_in, y_grid_size_in):\n",
    "    y = y_in*(-1)\n",
    "    # make sure that we do not overwrite the state of our player character in the grid\n",
    "    if not value == 1 and x == 0 and y == 0:\n",
    "        return grid\n",
    "    # resize grid in case we got uneven grid sizes\n",
    "    if x_grid_size_in%2==1:\n",
    "        x_grid_size = x_grid_size_in - 1\n",
    "    else:\n",
    "        x_grid_size = x_grid_size_in\n",
    "    if y_grid_size_in%2==1:\n",
    "        y_grid_size = y_grid_size_in -1\n",
    "    else:\n",
    "        y_grid_size = y_grid_size_in\n",
    "    x_pos = x + int(x_grid_size/2)\n",
    "    y_pos = y + int(y_grid_size/2)\n",
    "    grid[x_pos][y_pos] = value    \n",
    "    return grid\n",
    "\n",
    "# the teleport and recall functions dont work because it is not possible to send button presses and klicks\n",
    "# on the hud to the game\n",
    "# the win32api does work on another driver level than the game\n",
    "\n",
    "def teleport_to(screen_x, screen_y):\n",
    "    #press shift+s and klick a coordinate\n",
    "    win32api.keybd_event((0x10),0,0,0)\n",
    "    win32api.keybd_event((0x10),0 ,win32con.KEYEVENTF_KEYUP ,0)\n",
    "    time.sleep(0.05)\n",
    "    win32api.keybd_event((0x53),0,0,0)\n",
    "    win32api.keybd_event((0x53),0 ,win32con.KEYEVENTF_KEYUP ,0)\n",
    "    time.sleep(0.05)\n",
    "    #\n",
    "    #\n",
    "    win32api.SetCursorPos((screen_x,screen_y))\n",
    "    win32api.mouse_event(win32con.MOUSEEVENTF_MIDDLEDOWN,screen_x,screen_y,0,0)\n",
    "    win32api.mouse_event(win32con.MOUSEEVENTF_MIDDLEUP,screen_x,screen_y,0,0)\n",
    "def recall():\n",
    "    win32api.SetCursorPos((929,720))\n",
    "    win32api.mouse_event(win32con.MOUSEEVENTF_MIDDLEDOWN,920,720,0,0)\n",
    "    win32api.mouse_event(win32con.MOUSEEVENTF_MIDDLEUP,920,720,0,0)\n",
    "    #click(920,728 ,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    # The policy cointains the parameters used for making decisions and learning\n",
    "    # More on the paramters can be found int the pdf report in chapter 4\n",
    "    c_max_visible_dist = 20.0\n",
    "    c_close_dist_cutoff = 1.0 / 10.0\n",
    "    c_tower_safe_dist_cutoff = c_max_visible_dist / 3\n",
    "    c_max_hp = 1\n",
    "    # this parameter sets the time frame the bot should leave the base and go to lane\n",
    "    # this paramtere requires some further testing. the bot is not really smart enough yet to know where to go\n",
    "    # when the game starts. thats why the move to lane is hard coded and does not use this parameter for now\n",
    "    c_game_time_beginning_cutoff = 0  # set time in seconds until the normal game routine starts  #0.5 *60 / 5\n",
    "    policy_gradient = np.array([1, 1, 1, 1]).reshape( -1 )\n",
    "    delta_theta = []\n",
    "    delta_R = []\n",
    "    R_i = 1\n",
    "    alpha = 0.1\n",
    "    R_ref = 1\n",
    "    \n",
    "    logistic_func_x_scale = 6\n",
    "    \n",
    "    # theta is the policy parameter we learn for the different actions:\n",
    "    # atk minion, atk tower, move to goal, retreat\n",
    "    theta = np.zeros( (4,1) )\n",
    "    #other initial values for the parameters\n",
    "    #theta[0] = c_close_dist_cutoff\n",
    "    #theta[1] = 1\n",
    "    #theta[2] = 0.8 / (35*60/5)\n",
    "    #theta[3] = 0.2\n",
    "    theta[0] = 1\n",
    "    theta[1] = 1\n",
    "    theta[2] = 1\n",
    "    theta[3] = 1\n",
    "class State:\n",
    "    # This class defines the state in which the player is currently\n",
    "    hp = 1.0\n",
    "    cloest_minion_dist = 0\n",
    "    tower_dist = 0\n",
    "    # game time counted in iterations or steps\n",
    "    start_time = time.time()\n",
    "    game_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on how the learning and decision making works can be found in the pdf report in the repository in chapter 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intelligence\n",
    "def find_shortest_distance(unit_grid, unit_type):\n",
    "    distance = 10000 #no distance found\n",
    "    smallest_x = 0\n",
    "    smallest_y = 0\n",
    "    for i in range(3,len(unit_grid)-3):\n",
    "     for j in range(3,len(unit_grid[i])-3):\n",
    "        if unit_grid[i][j] == unit_type:#if minion found\n",
    "            cur_dist=math.sqrt(math.pow(i-int(len(unit_grid)/2),2)+math.pow(j-int(len(unit_grid[0])/2),2))\n",
    "            if cur_dist < distance:\n",
    "                distance = cur_dist\n",
    "                smallest_x = i-int(len(unit_grid)/2)\n",
    "                smallest_y = j-int(len(unit_grid[0])/2)\n",
    "    return [distance,smallest_x,-1*smallest_y]\n",
    "\n",
    "def minion_probability(closest_minion_distance,hp_frac):\n",
    "    #x: input to function\n",
    "    #1: max output value\n",
    "    #m: slope, calculated based on the max range in which an enemy is visible\n",
    "    #max_dist_enemy: maximum distance in which an enemy is\n",
    "    #x = closest_enemy_distance-closest_enemy_distance/5 #factor to increase minion threat level\n",
    "    #y = 7.954*math.pow(10,-5)*math.pow(x,4)-0.003558*math.pow(x,3)+0.05299*math.pow(x,2)-0.3257*x+0.9511\n",
    "    #print(\"closest_miniont: \" + str(closest_minion_distance))\n",
    "    #p = policy.theta[0]*(hp_frac/closest_minion_distance)\n",
    "    \n",
    "    p = policy.theta[0]*hp_frac*(1/( 1+exp( -2*(closest_minion_distance-3)) ) - 1/( 1+exp( -3*(closest_minion_distance-6)) ) )\n",
    "    if   p > 1: return 1\n",
    "    elif p < 0: return 0\n",
    "    else      : return p\n",
    "\n",
    "def tower_probability(closest_tower_distance, hp_frac):\n",
    "    #y = 0.05657865 + (1.01699 - 0.05657865)/(1 + math.pow((closest_tower/6.996663),5.975224))\n",
    "    #print(\"closest_tower: \" + str(closest_tower_distance))\n",
    "    \n",
    "    # reposition the curve the centre around [-6, +6]\n",
    "    #closest_tower_distance = closest_tower_distance - 0.17    # center the curve the offset or cutoff dist\n",
    "    #closest_tower_distance = closest_tower_distance - 0.5     # scale the range from  [0, 1] (or actually [-0.17, 1-0.17])\n",
    "    #closest_tower_distance = closest_tower_distance * 6*2\n",
    "    #print(\"closest_tower[-6, +6]: \" + str( (closest_tower_distance) ))\n",
    "    p = policy.theta[1]*hp_frac*(1/( 1+exp( -2*(closest_tower_distance+2)) ) - 1/( 1+exp( -2*(closest_tower_distance-4.6)) ) )\n",
    "    if   p > 1: return 1\n",
    "    elif p < 0: return 0\n",
    "    else      : return p\n",
    "\n",
    "def goal_probability():\n",
    "    #depending on game time. we assume that after a 60 min game the probabilty to approach is very high\n",
    "    #print(\"game_time_beginning_cutoff remaining:\" + str(state.game_time))\n",
    "    if state.game_time < policy.c_game_time_beginning_cutoff:\n",
    "        return 1\n",
    "    else:\n",
    "        #print(\"theta2: \" + str(policy.theta[2][0])+ \" game time: \" + str(state.game_time))\n",
    "        #p = policy.theta[2] * state.game_time\n",
    "        p = policy.theta[2]*0.0002228777*state.game_time + 0.204687\n",
    "        if   p > 1: return 1\n",
    "        elif p < 0: return 0\n",
    "        else      : return p\n",
    "        \n",
    "def retreat_probability(closest_enemy,hp_frac):\n",
    "    #p = exp( policy.theta[3] / hp_frac ) - 1   \n",
    "    #p = 0.9686364 - 1.217273*hp_frac + math.pow(0.3090909*hp_frac,2)\n",
    "    x = hp_frac * policy.theta[3]\n",
    "    x2 = closest_enemy * policy.theta[3]\n",
    "    p = 0.94 - 1.26*hp_frac + 0.4*math.pow(hp_frac,2) + (-0.11*x2+0.7)\n",
    "    if   p > 1: return 1\n",
    "    elif p < 0: return 0\n",
    "    else      : return p\n",
    "\n",
    "\n",
    "def decide_action(minion_prob, tower_prob, goal_prob, retreat_prob):\n",
    "   total_prob = minion_prob + tower_prob + goal_prob + retreat_prob\n",
    "\n",
    "   attack_minion_cutoff = minion_prob / total_prob\n",
    "   attack_tower_cutoff = (minion_prob + tower_prob) / total_prob\n",
    "   goal_cutoff = (minion_prob + tower_prob + goal_prob) / total_prob\n",
    "   retreat_cutoff = retreat_prob / total_prob\n",
    "\n",
    "   #roll random number and decide for one of the actions\n",
    "   rnd = random()\n",
    "   action = 0\n",
    "   if   rnd < attack_minion_cutoff: action = 0\n",
    "   elif rnd < attack_tower_cutoff:  action = 1\n",
    "   elif rnd < goal_cutoff:          action = 2\n",
    "   else:                            action = 3\n",
    "   return action\n",
    "\n",
    "\n",
    "def updateR_i(delta_theta_i):\n",
    "    R_i = policy.R_i\n",
    "    alpha = policy.alpha\n",
    "    \n",
    "    R_i = (1-alpha)*policy.R_i + alpha*np.dot(policy.policy_gradient, delta_theta_i)\n",
    "    policy.R_i = R_i\n",
    "    \n",
    "    return R_i\n",
    "\n",
    "def updateR_ref(reward):\n",
    "    R_ref = policy.R_ref\n",
    "    alpha = policy.alpha\n",
    "    \n",
    "    R_ref = (1-alpha)*R_ref + alpha*reward\n",
    "    policy.R_ref = R_ref\n",
    "    \n",
    "def estimate_policy_gradient_FD():\n",
    "    #TODO do we need ticks here?\n",
    "    #gets array with all actions in the last 5 seconds and the number of ticks/action done in the time frame\n",
    "    #policy_grad = inv(policy.delta_theta.T * policy.delta_theta) * policy.delta_theta.T * policy.delta_R \n",
    "    delta_theta = np.array(policy.delta_theta)\n",
    "    delta_R = np.array(policy.delta_R)\n",
    "    \n",
    "    policy_grad = ((inv((delta_theta.reshape((-1,4)).T).dot(delta_theta.reshape((-1,4))))\n",
    "                  ).dot(delta_theta.reshape((-1,4)).T)).dot(delta_R.reshape((-1,1))).reshape(4)\n",
    "    policy_grad_norm = np.linalg.norm(policy_grad.reshape(-1))\n",
    "    policy_grad /= policy_grad_norm\n",
    "    #print(\"pol_grad: \" + str(policy_grad))\n",
    "    #print(\"policy gradient shape\")\n",
    "    #print(policy_grad.shape)\n",
    "    policy.policy_gradient = policy_grad\n",
    "\n",
    "def perturbate_policy(delta_scale=1.0):\n",
    "    factor = np.random.random(4)*delta_scale\n",
    "    delta_theta_i = factor * np.array(policy.theta).reshape(4)\n",
    "    return delta_theta_i.tolist()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#=======move to lane code=====\n",
    "# hard coded behaviour to move to the middle lane and wait until the game begins\n",
    "\n",
    "#sct.get_pixels(mon)\n",
    "#screen = Image.frombytes('RGB', (sct.width, sct.height), sct.image)\n",
    "#screen = np.array(screen)\n",
    "#screen = cv2.resize(screen, (MONITOR_WIDTH, MONITOR_HEIGHT))\n",
    "#image_np = screen\n",
    "#cv2.imshow('AI_View', image_np)\n",
    "#for i in range(1,14,1):\n",
    "#    click(950,100,False)\n",
    "#    print(\"moving to lane\")\n",
    "#    time.sleep(2)\n",
    "#print(\"wait for battle to begin\")\n",
    "#time.sleep(55)\n",
    "#================================\n",
    "time.sleep(5)\n",
    "\n",
    "# variable to track when we found the champion for the last time. so if the champion moves to a spot where it is\n",
    "# not visible anymore it will move randomly until found again. or recall (if it would work ....)\n",
    "vayne_last_found = time.time()\n",
    "# Parameters for the rewards\n",
    "MINION_REWARD = 10\n",
    "TOWER_REWARD = -100\n",
    "BLOCKED = -999\n",
    "policy = Policy()\n",
    "state = State()\n",
    "# Initialize the rewards for 1 and 5 seconds average\n",
    "reward_1 = 0\n",
    "reward_5 = 0\n",
    "state.start_time = time.time()\n",
    "\n",
    "# game start stuff\n",
    "start = False\n",
    "start_time = time.time()\n",
    "\n",
    "# reset variables for decision making and reward calculation\n",
    "last_player_hp = 1\n",
    "attack_number_1 = 0\n",
    "attack_number_5 = 0\n",
    "hp_change_1 = 0\n",
    "hp_change_5 = 0\n",
    "last_reset_1 = time.time()\n",
    "last_reset_5 = time.time()\n",
    "tick_count_1 = 0\n",
    "tick_count_5 = 0\n",
    "game_start = time.time()\n",
    "\n",
    "# sct is used to get the pixels from the desktop\n",
    "sct = mss()\n",
    "# determine the size of the area to observe (the game)\n",
    "# mon = {'top': 0, 'left': 0, 'width': SCREEN_WIDTH, 'height': SCREEN_HEIGHT} for fullscreen\n",
    "mon = {'top': 0, 'left': 0, 'width': SCREEN_WIDTH, 'height': SCREEN_HEIGHT}\n",
    "# timer to see how fast the loop is running\n",
    "last_time = time.time()\n",
    "\n",
    "\n",
    "#fill the delta theta with values or else there is an error\n",
    "#delta_theta_i = perturbate_policy()\n",
    "#policy.delta_theta.append(delta_theta_i)\n",
    "\n",
    "\n",
    "print(\"theta0_minion;theta1_tower;theta2_approach;theta3_retreat\")\n",
    "\n",
    "\n",
    "\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    while True:\n",
    "      #print('=========Loop took: {} seconds'.format(time.time()-last_time))\n",
    "      last_time = time.time()\n",
    "      # update gametime\n",
    "      state.game_time = time.time() - state.start_time\n",
    "      # replaced by mss package\n",
    "      #screen = cv2.resize(grab_screen(region=(0,40,1024,768)), (800,450))\n",
    "      #screen = np.array(ImageGrab.grab(bbox=(0,40,1024,768)))\n",
    "      # get screen recording and resize it to 800x450 pixels for output\n",
    "      sct.get_pixels(mon)\n",
    "      screen = Image.frombytes('RGB', (sct.width, sct.height), sct.image)\n",
    "      screen = np.array(screen)\n",
    "      screen = cv2.resize(screen, (MONITOR_WIDTH, MONITOR_HEIGHT))\n",
    "      image_np = screen\n",
    "      # ===================tensorflow template code===============\n",
    "      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "      image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "      # Each box represents a part of the image where a particular object was detected.\n",
    "      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "      # Each score represent how level of confidence for each of the objects.\n",
    "      # Score is shown on the result image, together with the class label.\n",
    "      scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "      classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "      num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "      # Actual detection.\n",
    "      (boxes, scores, classes, num_detections) = sess.run(\n",
    "          [boxes, scores, classes, num_detections],\n",
    "          feed_dict={image_tensor: image_np_expanded})                   \n",
    "      # Visualization of the results of a detection\n",
    "      vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          np.squeeze(boxes),\n",
    "          np.squeeze(classes).astype(np.int32),\n",
    "          np.squeeze(scores),\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          min_score_thresh=0.30,\n",
    "          line_thickness=4)\n",
    "\n",
    "      #===================Player Character=======================\n",
    "      # detect hp:\n",
    "      # read the number of green vs non green pixels from the health bar (hard coded for )\n",
    "      hp_y_min = 572\n",
    "      hp_y_max = hp_y_min + 8\n",
    "      hp_x_min = int(MONITOR_WIDTH/2-115)\n",
    "      hp_x_max = int(MONITOR_WIDTH/2+56)  \n",
    "      # print the dimensions and positions of the square for debuging purpsoes\n",
    "      #print(image_np[hp_y_min][hp_x_min])\n",
    "      #print(image_np[hp_y_max][hp_x_max])\n",
    "      #rectangle indicator to see the region where the hp are read from\n",
    "      cv2.rectangle(image_np, (hp_x_min, hp_y_min),(hp_x_max,hp_y_max),(0, 0, 255), 1)\n",
    "      green_pixels = 0\n",
    "      for x_hp in range(hp_x_min,hp_x_max,1):\n",
    "            for y_hp in range(hp_y_min,hp_y_max,1):\n",
    "                if image_np[y_hp][x_hp][1] > 25:\n",
    "                    green_pixels = green_pixels + 1\n",
    "      #print(\"Player HP: \" + str(green_pixels/((hp_x_max-hp_x_min-1)*(hp_y_max-hp_y_min-1))))\n",
    "      playerHP = green_pixels/((hp_x_max-hp_x_min-1)*(hp_y_max-hp_y_min-1))\n",
    "    \n",
    "      # reset various game variables\n",
    "      gameover = False # did the player die?\n",
    "      vayne_found = False # did we found the player character?\n",
    "      minion_count = 0 # how many enemy minions have been detected\n",
    "      tower_count = 0 # how many towers are around?\n",
    "      try:\n",
    "          if vayne_last_found + 10 < time.time():\n",
    "              # recall if the hap are low and then move back to lane\n",
    "              # unfortunately the recall function is not working so we just click randomly until we find the\n",
    "              # the player character again\n",
    "              #if playerHP <= 0.15:\n",
    "                  #recall to make sure we are in base then, doesnt work because i cant sent keystrokes or hud clicks ....\n",
    "                  #recall()\n",
    "                  #time.sleep(50)#wait death timerand move back to lane\n",
    "                  #for i in range(1,14,1):\n",
    "                  #    click(950,100,False)\n",
    "                  #    print(\"moving to lane\")\n",
    "                  #    time.sleep(2)\n",
    "                  #else:\n",
    "              click_state(randint(-2,2),randint(-2,2),w,h,player_origin,False);\n",
    "      except NameError:\n",
    "          print('player_origin not set yet')\n",
    "      for i,b in enumerate(boxes[0]):\n",
    "          if scores[0][i] >= 0.3:\n",
    "              #Vaynefound makes sure that the whole grid + decision proces only executed once per detection in case of a multi detection\n",
    "              if classes[0][i] == 1 and vayne_found == False:\n",
    "                  vayne_found = True;\n",
    "                  vayne_last_found = time.time()\n",
    "                  player_position = find_box_xy(boxes[0][i])\n",
    "                  # draw a circle where the center of the player character is (not the center of the rectangle\n",
    "                  # but some arbitrary spot between the legs of the model that was found experimentally)\n",
    "                  player_origin = [int(player_position[1]*MONITOR_WIDTH), int(player_position[0]*MONITOR_HEIGHT)]\n",
    "                  cv2.circle(image_np, (player_origin[0], player_origin[1]), 2, (0, 0, 255), 2)\n",
    "                  #calculate the origin of the center rectangle under the character\n",
    "                  origin = [int(player_origin[0]+9)-int(player_position[2]*MONITOR_WIDTH/3), player_origin[1]-15,int(player_origin[0])+20, int(player_origin[1])+18]        \n",
    "                  cv2.rectangle(image_np,(origin[0], origin[1]),(origin[2], origin[3]), (0, 255, 0), 1)\n",
    "                  #create draw states\n",
    "                  w = int((origin[2] - origin[0]))\n",
    "                  h = int((origin[3] - origin[1]))\n",
    "                  grid_x = int(SCREEN_WIDTH/w)\n",
    "                  grid_y = int(SCREEN_HEIGHT/h)\n",
    "                  dim = (grid_x,grid_y)\n",
    "                  unit_grid = np.zeros(dim) \n",
    "                  #set the origin of the grid to be the player character represented by a 1\n",
    "                  #the grid goes from -grid_x/2 to grid_x/2 because the player is the center and everything left and under it is negative\n",
    "                  unit_grid = set_array_pos(unit_grid,0,0,1,grid_x,grid_y)\n",
    "                  #=======DETECT OBJECTS=======\n",
    "                  #2.Minions\n",
    "                  for j,k in enumerate(boxes[0]):\n",
    "                      if scores[0][j] >= 0.3:\n",
    "                          if classes[0][j] == 2:\n",
    "                              minion_count = minion_count + 1\n",
    "                              cur_minion_position = find_box_xy(boxes[0][j])\n",
    "                              cur_minion_origin = [int(cur_minion_position[1]*MONITOR_WIDTH), int(cur_minion_position[0]*MONITOR_HEIGHT)]\n",
    "                              cv2.circle(image_np, (cur_minion_origin[0], cur_minion_origin[1]), 2, (0, 0, 255), 2)\n",
    "                              #Find in which state in the unit_grid the minion is\n",
    "                              cur_minion_state = find_object_state(unit_grid, cur_minion_origin, player_origin, w, h)\n",
    "                              unit_grid = set_array_pos(unit_grid,cur_minion_state[0],cur_minion_state[1],2,grid_x,grid_y)\n",
    "                          #3.Towers\n",
    "                          if classes[0][j] == 3:\n",
    "                              tower_count = tower_count + 1\n",
    "                              cur_tower_position = find_box_xy(boxes[0][j])\n",
    "                              cur_tower_origin = [int(cur_tower_position[1]*MONITOR_WIDTH), int(cur_tower_position[0]*MONITOR_HEIGHT)]\n",
    "                              cv2.circle(image_np, (cur_tower_origin[0], cur_tower_origin[1]), 2, (0, 0, 255), 4)\n",
    "                              cur_tower_state = find_object_state(unit_grid, cur_tower_origin, player_origin, w, h)\n",
    "                              unit_grid = set_array_pos(unit_grid,cur_tower_state[0],cur_tower_state[1],3,grid_x,grid_y)                 \n",
    "\n",
    "                  #=============================\n",
    "                  # Visualize Objects and generate reward matrix (each cell contains a certain reward)\n",
    "                  # not really needed anymore but the code is still used to draw the objects in the grid\n",
    "                  dim = (grid_x,grid_y)\n",
    "                  reward_grid = np.zeros(dim)\n",
    "                  for x in range(len(unit_grid)):\n",
    "                      for y in range(len(unit_grid[0])):\n",
    "                          #fill reward grid at the same time\n",
    "                          if unit_grid[x][y] == 1:\n",
    "                              #set the state at which the player is as blocked (-999 reward)\n",
    "                              reward_grid[x][y] = BLOCKED\n",
    "                              #===Paint player rectangle blue\n",
    "                              cv2.rectangle(image_np,(origin[0]+w*x-int(grid_x/2)*w, origin[1]+h*y-int(grid_y/2)*h),(origin[2]+w*x-int(grid_x/2)*w, origin[3]+h*y-int(grid_y/2)*h), (0, 255, 0), 1)\n",
    "                              cv2.rectangle(image_np,(origin[0]+2+w*x-int(grid_x/2)*w, origin[1]+2+h*y-int(grid_y/2)*h),(origin[2]-2+w*x-int(grid_x/2)*w, origin[3]-2+h*y-int(grid_y/2)*h), (0, 0, 255), 2)\n",
    "                          elif unit_grid[x][y] == 2:\n",
    "                              reward_grid[x][y] = MINION_REWARD\n",
    "                              #===MINIONS\n",
    "                              cv2.rectangle(image_np,(origin[0]+2+w*x-int(grid_x/2)*w, origin[1]+2+h*y-int(grid_y/2)*h),(origin[2]-2+w*x-int(grid_x/2)*w, origin[3]-2+h*y-int(grid_y/2)*h), (255, 0, 0), 2)\n",
    "                          elif unit_grid[x][y] == 3:\n",
    "                              reward_grid[x][y] = TOWER_REWARD\n",
    "                              #===TOWER\n",
    "                              cv2.rectangle(image_np,(origin[0]+2+w*x-int(grid_x/2)*w, origin[1]+2+h*y-int(grid_y/2)*h),(origin[2]-2+w*x-int(grid_x/2)*w, origin[3]-2+h*y-int(grid_y/2)*h), (255, 255, 255), 2)\n",
    "                          else:\n",
    "                              # draw empty rectangles green\n",
    "                              cv2.rectangle(image_np,(origin[0]+w*x-int(grid_x/2)*w, origin[1]+h*y-int(grid_y/2)*h),(origin[2]+w*x-int(grid_x/2)*w, origin[3]+h*y-int(grid_y/2)*h), (0, 255, 0), 1)\n",
    "\n",
    "\n",
    "                  #=======Hard Coded Decision Making/Outdated========\n",
    "                  #if shortest_distance <= 5.0:\n",
    "                  #      #print('too close! run!')\n",
    "                  #      cv2.rectangle(image_np,(0,0), (w*5,h*5), (0, 255, 0), -1)\n",
    "                  #      if closest_x > 0 and closest_y > 0:\n",
    "                  #          click_state(-closest_x/abs(closest_x), -closest_y/abs(closest_y),w,h,player_origin,False)\n",
    "                  #elif shortest_distance < 8.0 and shortest_distance > 5.0:\n",
    "                  #      #print('attack!')\n",
    "                  #      cv2.rectangle(image_np,(0,0), (w*3,h*3), (0, 0, 255), -1)\n",
    "                  #      click_state(closest_x, closest_y,w,h,player_origin,True)\n",
    "                  #      time.sleep(1)\n",
    "                  #elif shortest_distance > 8.0 and minion_count > 0:\n",
    "                  #      cv2.rectangle(image_np,(0,0), (w*3,h*3), (255, 0, 0), -1)\n",
    "                  #      #print('not in range, approaching!')\n",
    "                  #      click_state(closest_x, closest_y,w,h,player_origin,False)\n",
    "                  #else:\n",
    "                  #      cv2.rectangle(image_np,(0,0), (w*3,h*3), (0, 0, 0), -1)\n",
    "                  #      #print('no enemies found, run towards enemy base!')\n",
    "                  #      click_state(1, 1,w,h,player_origin,False)\n",
    "\n",
    "\n",
    "                  #=======Calculate the threat values of objects==========\n",
    "                  #get the info based on which the decisions shall be made\n",
    "                  [shortest_distance_minion, closest_minion_x, closest_minion_y] = find_shortest_distance(unit_grid, 2)\n",
    "                  [shortest_distance_tower, closest_tower_x, closest_tower_y] = find_shortest_distance(unit_grid, 3)\n",
    "                  #shortest_distance_tower = shortest_distance_tower/math.sqrt(math.pow((grid_x/2),2)+math.pow((grid_y/2),2))\n",
    "                  #print(\"closest minion\" + str(shortest_distance_minion))\n",
    "                  # calculate probabilites for each action\n",
    "                  attack_tower_prob = tower_probability(shortest_distance_tower, playerHP)\n",
    "                  approach_enemy_base_prob =  goal_probability()\n",
    "                  retreat_prob = retreat_probability(shortest_distance_minion, playerHP)\n",
    "                  if shortest_distance_minion == 10000: #if there are no minions\n",
    "                      attack_minion_prob = 0\n",
    "                      approach_enemy_base_prob = approach_enemy_base_prob + 0.1\n",
    "                  else:\n",
    "                      #shortest_distance_minion = shortest_distance_minion/math.sqrt(math.pow((grid_x/2),2)+math.pow((grid_y/2),2))\n",
    "                      attack_minion_prob = minion_probability(shortest_distance_minion, playerHP)\n",
    "\n",
    "                  \n",
    "                  #print(\"min: \" + str(attack_minion_prob) + \" tow: \" + str(attack_tower_prob) + \" appr: \" + str(approach_enemy_base_prob) + \" retr: \" + str(retreat_prob))\n",
    "                  #print(\"attack_tower_prob: \" + str(attack_tower_prob))\n",
    "                  #print(\"approach_enemy_base_prob: \" + str(approach_enemy_base_prob))\n",
    "                  #print(\"retreat_prob: \" + str(retreat_prob))\n",
    "\n",
    "                  #=======Make decision which action to take and to which state====\n",
    "                  #[state_x, state_y, action] = make_decision(playerHP, unit_grid, minion_value, tower_value);\n",
    "                  action = decide_action(attack_minion_prob, attack_tower_prob, approach_enemy_base_prob, retreat_prob)\n",
    "                  #print(\"selected action: \" + str(action))\n",
    "                  ##=======Execute the action===========\n",
    "                  if action == 0:#Attack Minion\n",
    "                      click_state(closest_minion_x, closest_minion_y, w, h, player_origin, True)\n",
    "                      time.sleep(0.5)\n",
    "                      attack_number_1 = attack_number_1 + 2\n",
    "                      attack_number_5 = attack_number_5 + 2\n",
    "                  elif action == 1:#Attack Tower\n",
    "                      click_state(closest_tower_x, closest_tower_y, w, h, player_origin, True)\n",
    "                      time.sleep(0.5)\n",
    "                      attack_number_1 = attack_number_1 + 4\n",
    "                      attack_number_5 = attack_number_5 + 4\n",
    "                  elif action == 2:#Approach\n",
    "                      #Just move to the top right side\n",
    "                      click_state(1, 1, w, h, player_origin, False)\n",
    "                  else: #run away from the closest enemy by just running the the oposite direction as the enemy minion is\n",
    "                      if shortest_distance_minion == 10000 or closest_minion_x <= 0 or closest_minion_y <= 0:\n",
    "                          click_state(-1, -1, w, h, player_origin, False)\n",
    "                      else:\n",
    "                          click_state(2*(-closest_minion_x/abs(closest_minion_x)), 2*(-closest_minion_y/abs(closest_minion_y)), w, h, player_origin, False);\n",
    "\n",
    "                  #======Calculate Reward and feed it back======\n",
    "                  # This part of the algorithm is roughly described in the pdf report in chapter 4.5\n",
    "                  hp_change_1 = hp_change_1 + (- playerHP + last_player_hp) #add up the hp change until reset\n",
    "                  hp_change_5 = hp_change_5 +  (- playerHP + last_player_hp)\n",
    "                  tick_count_1 = tick_count_1 + 1\n",
    "                  tick_count_5 = tick_count_5 + 1\n",
    "                  last_player_hp = playerHP #update the current hp for the next loop\n",
    "                  #other params: attack_number_1/5 number of attacks since last reset\n",
    "\n",
    "                  cv2.rectangle(image_np,(0,0), (102,154), (255, 255, 255), 2)\n",
    "                  #minion\n",
    "                  cv2.rectangle(image_np,(2,2), (int(100*attack_minion_prob),30), (0, 0, 125), -1)\n",
    "                  cv2.rectangle(image_np,(2,32), (int(100*attack_tower_prob),60), (0, 0, 255), -1)\n",
    "                  cv2.rectangle(image_np,(2,62), (int(100*approach_enemy_base_prob),90), (255, 0, 0), -1)\n",
    "                  cv2.rectangle(image_np,(2,92), (int(100*retreat_prob),122), (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "\n",
    "                  #calculate reward average over the last 1 and 5 seconds                  \n",
    "                  #reset every 1 sec\n",
    "                  #if last_reset_1 + 1 < time.time():  \n",
    "                  #    reward_1 = 1-(hp_change_1/100) + attack_number_1/tick_count_1\n",
    "                  #    hp_change_1 = 0\n",
    "                  #    attack_number_1 = 0\n",
    "                  #    print(\"reward_1: \" + str(reward_1))\n",
    "                  #    tick_count_1 = 0\n",
    "                  #    last_reset_1 = time.time()\n",
    "                  ##reset every 5 sec\n",
    "                  #if last_reset_5 + 5 < time.time():  \n",
    "                  reward_5 = (-20*hp_change_5) + attack_number_5 + (state.game_time/100)#/tick_count_5\n",
    "                  cv2.rectangle(image_np,(51,122), (51+int((reward_5*6)),152), (255, 255, 255), -1)\n",
    "                  cv2.rectangle(image_np,(51,122), (51,152), (0, 0, 255), -1)\n",
    "                  #print(\"reward: \" + str(reward_5) + \" hp_change_5: \" + str(hp_change_5) + \" attack_number_5: \" + str(attack_number_5))\n",
    "                  hp_change_5 = 0\n",
    "                  attack_number_5 = 0\n",
    "                  #print(\" reward_5: \" + str(reward_5))\n",
    "                  if last_reset_5 + 5 < time.time():  \n",
    "                      #policy gradient estimation\n",
    "                      estimate_policy_gradient_FD()\n",
    "                      #print(policy.theta)\n",
    "                      #print(policy.policy_gradient.reshape(4))\n",
    "                      policy.theta += policy.policy_gradient.reshape(4,1)*0.01\n",
    "                      #care if any value is < 0\n",
    "                      if policy.theta[2] < 0:\n",
    "                          policy.theta[2] = 0.0001\n",
    "                      tick_count_5 = 0\n",
    "                      policy.delta_R = []\n",
    "                      policy.delta_theta = []\n",
    "                      last_reset_5 = time.time()\n",
    "\n",
    "                  #===============FEEDBACK==================\n",
    "                  delta_theta_i = perturbate_policy()\n",
    "                  policy.delta_theta.append(delta_theta_i)\n",
    "                  updateR_i(delta_theta_i)\n",
    "                  updateR_ref(reward_5)\n",
    "\n",
    "                  delta_R = policy.R_i - policy.R_ref\n",
    "                  #append the current action to delta_R[i]\n",
    "                  policy.delta_R.append(delta_R)\n",
    "                  print(str(policy.theta[0][0])+\";\"+str(policy.theta[1][0])+\";\"+str(policy.theta[2][0])+\";\"+str(policy.theta[3][0])+\";\"+str(reward_5))\n",
    "                  #todo: print the reward and r_ref and see why its just declining\n",
    "                  #print(\"======================================================================\")               \n",
    "      # ==================OPEN CV visualization=================\n",
    "      cv2.imshow('AI_View', image_np)\n",
    "      if (cv2.waitKey(25) & 0xFF == ord('q')):\n",
    "          cv2.destroyAllWindows()\n",
    "          break\n",
    "      if gameover == True:\n",
    "          print('gameover')\n",
    "          cv2.destroyAllWindows()\n",
    "          break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot test for plotting thetas continuously for learning monitoring\n",
    "\n",
    "#import matplotlib.pylab as pylab\n",
    "#%matplotlib notebook\n",
    "#def update_line(hl,x,p):\n",
    "#    hl.set_xdata(x)\n",
    "#    hl.set_ydata(p)\n",
    "#    plt.draw()\n",
    "#    plt.flush_events()\n",
    "#x = np.arange(-6,6,0.1)\n",
    "#playerHP_plot = 1\n",
    "#thet = 1##\n",
    "#\n",
    "##fig=plt.figure()\n",
    "#plt.show()\n",
    "#for i in range(1,10):\n",
    "#    p2 = i*playerHP_plot*(1/(1+np.exp(-2*(x+1)))-1/(1+np.exp(-10*(x-3))))\n",
    "#    plt.plot(x,p2)\n",
    "#    plt.draw()\n",
    " ##   fig.canvas.flush_events()\n",
    " #   time.sleep(1)\n",
    "    #plt.gcf().clear()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#while True:\n",
    "#    p = np.exp(x)#policy.theta[1]*playerHP_plot*(1/(1+np.exp(-2*(x+1)))-1/(1+np.exp(-10*(x-3))))\n",
    "#    t, = pylab.plot(x,p)\n",
    "#    time.sleep(1)\n",
    "#    thet = 1\n",
    "#    plt.show()\n",
    "#p = policy.theta[1]*playerHP*(1/( 1+exp( -2*(x+1)) ) - 1/( 1+exp( -10*(x-3)) ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
